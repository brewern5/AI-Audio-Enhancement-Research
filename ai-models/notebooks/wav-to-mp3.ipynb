{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13425830,"sourceType":"datasetVersion","datasetId":8521464}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pydub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T16:48:47.907827Z","iopub.execute_input":"2025-10-18T16:48:47.908172Z","iopub.status.idle":"2025-10-18T16:48:54.193198Z","shell.execute_reply.started":"2025-10-18T16:48:47.908141Z","shell.execute_reply":"2025-10-18T16:48:54.191727Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Audio file conversion script (.wav to mp3)","metadata":{}},{"cell_type":"code","source":"import os\nfrom pydub import AudioSegment\n\n#Two versions of audio created\ninput_dir = \"/kaggle/input/smalldataset\" \noutput_dir = \"/kaggle/working/mp3_converted\"\n#output_dir = \"/kaggle/working/mp3_converted_192k\"\n\n# Desired MP3 bitrate (e.g., '128k', '192k', '320k')\nbitrate = \"128k\"\n#bitrate = \"192k\"\n\n# --- Conversion Logic ---\nprint(f\"Starting conversion of WAV files from: {input_dir}\")\n\n# 1. Create the output directory if it doesn't exist\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n    print(f\"Created output directory: {output_dir}\")\n\n# 2. List all files in the input directory\ntry:\n    files = os.listdir(input_dir)\nexcept FileNotFoundError:\n    print(f\"ERROR: Input directory not found: {input_dir}\")\n    print(\"Please make sure you have attached your dataset to the notebook and the path is correct.\")\n    files = []\n\nconverted_count = 0\n# 3. Loop through files and convert the .wav files\nfor filename in files:\n    if filename.lower().endswith(\".wav\"):\n        try:\n            # Construct full file paths\n            wav_path = os.path.join(input_dir, filename)\n            mp3_filename = os.path.splitext(filename)[0] + \".mp3\"\n            mp3_path = os.path.join(output_dir, mp3_filename)\n            \n            # Load the WAV file\n            audio = AudioSegment.from_wav(wav_path)\n            \n            # Export as MP3 with specified bitrate\n            print(f\"Converting {filename} to MP3...\")\n            audio.export(mp3_path, format=\"mp3\", bitrate=bitrate)\n            converted_count += 1\n            \n        except Exception as e:\n            print(f\"Could not convert {filename}. Error: {e}\")\n\nprint(f\"\\nConversion complete. Converted {converted_count} files.\")\nprint(f\"MP3 files are saved in: {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T01:12:19.244939Z","iopub.execute_input":"2025-10-20T01:12:19.245672Z","iopub.status.idle":"2025-10-20T01:13:08.748707Z","shell.execute_reply.started":"2025-10-20T01:12:19.245641Z","shell.execute_reply":"2025-10-20T01:13:08.747718Z"}},"outputs":[{"name":"stdout","text":"Starting conversion of WAV files from: /kaggle/input/smalldataset\nCreated output directory: /kaggle/working/mp3_converted\nConverting a3.wav to MP3...\nConverting a1.wav to MP3...\nConverting a4.wav to MP3...\nConverting a2.wav to MP3...\n\nConversion complete. Converted 4 files.\nMP3 files are saved in: /kaggle/working/mp3_converted\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Data manifest file in csv: will pair each MP3 with its corresponding WAV file","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# --- Configuration ---\n# Define the paths to your data directories in Kaggle\nwav_dir = \"/kaggle/input/smalldataset/\"\nmp3_128k_dir = \"/kaggle/working/mp3_converted/\"\nmp3_192k_dir = \"/kaggle/working/mp3_converted_192k/\"\n\n# Output path for the final CSV file\noutput_csv_path = \"/kaggle/working/dataset_manifest.csv\"\n\n# --- Logic to Create Pairs ---\ndata_pairs = []\n\n# We'll use the original WAV files as the source of truth\nprint(f\"Scanning for WAV files in: {wav_dir}\")\nfor wav_filename in os.listdir(wav_dir):\n    if wav_filename.lower().endswith(\".wav\"):\n        base_name = os.path.splitext(wav_filename)[0]\n        wav_path = os.path.join(wav_dir, wav_filename)\n\n        # --- Pair with 128k MP3 ---\n        mp3_128k_path = os.path.join(mp3_128k_dir, f\"{base_name}.mp3\")\n        if os.path.exists(mp3_128k_path):\n            data_pairs.append({\n                \"input_path\": mp3_128k_path,\n                \"target_path\": wav_path,\n                \"bitrate_kbps\": 128,\n                \"original_id\": base_name\n            })\n        else:\n            print(f\"Warning: Could not find matching 128k MP3 for {wav_filename}\")\n\n        # --- Pair with 192k MP3 ---\n        mp3_192k_path = os.path.join(mp3_192k_dir, f\"{base_name}.mp3\")\n        if os.path.exists(mp3_192k_path):\n            data_pairs.append({\n                \"input_path\": mp3_192k_path,\n                \"target_path\": wav_path,\n                \"bitrate_kbps\": 192,\n                \"original_id\": base_name\n            })\n        else:\n            print(f\"Warning: Could not find matching 192k MP3 for {wav_filename}\")\n\n# --- Create and Save the DataFrame ---\nif data_pairs:\n    df = pd.DataFrame(data_pairs)\n    df.to_csv(output_csv_path, index=False)\n    print(f\"\\nSuccessfully created manifest file with {len(df)} pairs.\")\n    print(f\"CSV saved to: {output_csv_path}\")\n    \n    # Display the first few rows of the created table\n    print(\"\\n--- CSV Preview ---\")\n    print(df.head())\nelse:\n    print(\"\\nNo data pairs were created. Please check your directory paths.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T01:13:30.640787Z","iopub.execute_input":"2025-10-20T01:13:30.641135Z","iopub.status.idle":"2025-10-20T01:13:30.654916Z","shell.execute_reply.started":"2025-10-20T01:13:30.641081Z","shell.execute_reply":"2025-10-20T01:13:30.654147Z"}},"outputs":[{"name":"stdout","text":"Scanning for WAV files in: /kaggle/input/smalldataset/\n\nSuccessfully created manifest file with 8 pairs.\nCSV saved to: /kaggle/working/dataset_manifest.csv\n\n--- CSV Preview ---\n                                  input_path  \\\n0       /kaggle/working/mp3_converted/a3.mp3   \n1  /kaggle/working/mp3_converted_192k/a3.mp3   \n2       /kaggle/working/mp3_converted/a1.mp3   \n3  /kaggle/working/mp3_converted_192k/a1.mp3   \n4       /kaggle/working/mp3_converted/a4.mp3   \n\n                         target_path  bitrate_kbps original_id  \n0  /kaggle/input/smalldataset/a3.wav           128          a3  \n1  /kaggle/input/smalldataset/a3.wav           192          a3  \n2  /kaggle/input/smalldataset/a1.wav           128          a1  \n3  /kaggle/input/smalldataset/a1.wav           192          a1  \n4  /kaggle/input/smalldataset/a4.wav           128          a4  \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install librosa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T17:49:33.448178Z","iopub.execute_input":"2025-10-18T17:49:33.448537Z","iopub.status.idle":"2025-10-18T17:49:37.520103Z","shell.execute_reply.started":"2025-10-18T17:49:33.448512Z","shell.execute_reply":"2025-10-18T17:49:37.519107Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Data Loader:\nThe core of the data loading process is a custom Dataset class. This class tells PyTorch three essential things:\n\n__init__: How to initialize the dataset (e.g., by loading the CSV).\n\n__len__: How many total items are in the dataset (the number of rows in the CSV).\n\n__getitem__: How to get a single item (one input/target pair) from the dataset.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchaudio\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F # We need this for the padding function\n\n# The Dataset class can remain the same as before\nclass AudioUpscalingDataset(Dataset):\n    def __init__(self, manifest_path, target_sample_rate=44100):\n        self.manifest = pd.read_csv(manifest_path)\n        self.target_sample_rate = target_sample_rate\n\n    def __len__(self):\n        return len(self.manifest)\n\n    def __getitem__(self, idx):\n        input_path = self.manifest.iloc[idx]['input_path']\n        target_path = self.manifest.iloc[idx]['target_path']\n        \n        try:\n            input_waveform, orig_sr_input = torchaudio.load(input_path)\n            target_waveform, orig_sr_target = torchaudio.load(target_path)\n        except Exception as e:\n            print(f\"Error loading files at index {idx}: {e}\")\n            return torch.zeros(1, 1), torch.zeros(1, 1) # Return minimal tensor on error\n\n        if orig_sr_input != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(orig_sr_input, self.target_sample_rate)\n            input_waveform = resampler(input_waveform)\n        \n        if orig_sr_target != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(orig_sr_target, self.target_sample_rate)\n            target_waveform = resampler(target_waveform)\n\n        # We no longer need to manually fix lengths here. The collate_fn will handle it.\n        return input_waveform, target_waveform\n\n# --- NEW: The Custom Collate Function ---\ndef pad_collate_fn(batch):\n    \"\"\"\n    Pads audio samples in a batch to the length of the longest sample.\n    Args:\n        batch: A list of tuples, where each tuple is (input_waveform, target_waveform).\n    \"\"\"\n    # Separate the inputs and targets\n    inputs, targets = zip(*batch)\n\n    # Find the maximum length in the batch for inputs\n    max_input_len = max(w.shape[1] for w in inputs)\n    # Find the maximum length in the batch for targets\n    max_target_len = max(w.shape[1] for w in targets)\n    # Use the overall max length to be safe\n    max_len = max(max_input_len, max_target_len)\n    \n    # Pad all inputs to the max_len\n    # `pad` arguments are (left, right, top, bottom) for 2D tensors\n    padded_inputs = torch.stack([\n        F.pad(w, (0, max_len - w.shape[1])) for w in inputs\n    ])\n\n    # Pad all targets to the max_len\n    padded_targets = torch.stack([\n        F.pad(w, (0, max_len - w.shape[1])) for w in targets\n    ])\n\n    return padded_inputs, padded_targets\n\n# --- UPDATED: Using the DataLoader ---\n\n# 1. Define the path to your manifest file\nmanifest_file = \"/kaggle/working/dataset_manifest.csv\"\n\n# 2. Create an instance of your custom Dataset\naudio_dataset = AudioUpscalingDataset(manifest_path=manifest_file, target_sample_rate=44100)\n\n# 3. Create the DataLoader, NOW WITH THE CUSTOM COLLATE FUNCTION\nbatch_size = 4\ntrain_loader = DataLoader(\n    audio_dataset, \n    batch_size=batch_size, \n    shuffle=True,\n    collate_fn=pad_collate_fn  # <-- This is the crucial change!\n)\n\n# 4. Now, this loop will work without errors\nprint(\"Running DataLoader with padding collate function...\")\nfor i, (inputs, targets) in enumerate(train_loader):\n    print(f\"Batch {i+1}:\")\n    print(f\"  Input batch shape: {inputs.shape}\")\n    print(f\"  Target batch shape: {targets.shape}\")\n    # All tensors in a batch will now have the same length.\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T01:14:11.695031Z","iopub.execute_input":"2025-10-20T01:14:11.695381Z","iopub.status.idle":"2025-10-20T01:14:20.864808Z","shell.execute_reply.started":"2025-10-20T01:14:11.695355Z","shell.execute_reply":"2025-10-20T01:14:20.864051Z"}},"outputs":[{"name":"stdout","text":"Running DataLoader with padding collate function...\nBatch 1:\n  Input batch shape: torch.Size([4, 2, 23653938])\n  Target batch shape: torch.Size([4, 2, 23653938])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"U-Net model with 1D convolutions","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ConvBlock(nn.Module):\n    \"\"\"A single encoder block: Conv1D -> BatchNorm -> LeakyReLU\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=15, stride=1, padding=7):\n        super().__init__()\n        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n        self.bn = nn.BatchNorm1d(out_channels)\n        self.relu = nn.LeakyReLU(0.2)\n\n    def forward(self, x):\n        return self.relu(self.bn(self.conv(x)))\n\nclass UpConvBlock(nn.Module):\n    \"\"\"A single decoder block: Upsample -> Conv1D -> BatchNorm -> LeakyReLU\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1, padding=2):\n        super().__init__()\n        # Using ConvTranspose1d to upsample\n        self.upconv = nn.ConvTranspose1d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n        self.bn = nn.BatchNorm1d(out_channels)\n        self.relu = nn.LeakyReLU(0.2)\n\n    def forward(self, x):\n        return self.relu(self.bn(self.upconv(x)))\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=2, out_channels=2):\n        super().__init__()\n\n        # --- Encoder ---\n        self.enc1 = ConvBlock(in_channels, 16)\n        self.pool1 = nn.AvgPool1d(kernel_size=2, stride=2) # Downsample\n        self.enc2 = ConvBlock(16, 32)\n        self.pool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n        self.enc3 = ConvBlock(32, 64)\n        self.pool3 = nn.AvgPool1d(kernel_size=2, stride=2)\n\n        # --- Bottleneck ---\n        self.bottleneck = ConvBlock(64, 128)\n\n        # --- Decoder ---\n        # The in_channels for the decoder is double because of the skip connection concatenation\n        self.upconv3 = UpConvBlock(128, 64)\n        self.dec3 = ConvBlock(128, 64) # 64 from upconv + 64 from enc3 skip\n        self.upconv2 = UpConvBlock(64, 32)\n        self.dec2 = ConvBlock(64, 32)  # 32 from upconv + 32 from enc2 skip\n        self.upconv1 = UpConvBlock(32, 16)\n        self.dec1 = ConvBlock(32, 16)  # 16 from upconv + 16 from enc1 skip\n        \n        # --- Output Layer ---\n        self.final_conv = nn.Conv1d(16, out_channels, kernel_size=1)\n        self.final_tanh = nn.Tanh()\n\n    def forward(self, x):\n        # --- Encoder Path ---\n        e1 = self.enc1(x)\n        p1 = self.pool1(e1)\n        e2 = self.enc2(p1)\n        p2 = self.pool2(e2)\n        e3 = self.enc3(p2)\n        p3 = self.pool3(e3)\n\n        # --- Bottleneck ---\n        b = self.bottleneck(p3)\n\n        # --- Decoder Path with Skip Connections ---\n        u3 = self.upconv3(b)\n        # Concatenate skip connection from encoder\n        skip3 = torch.cat([u3, e3], dim=1)\n        d3 = self.dec3(skip3)\n\n        u2 = self.upconv2(d3)\n        skip2 = torch.cat([u2, e2], dim=1)\n        d2 = self.dec2(skip2)\n\n        u1 = self.upconv1(d2)\n        skip1 = torch.cat([u1, e1], dim=1)\n        d1 = self.dec1(skip1)\n        \n        # --- Final Output ---\n        out = self.final_conv(d1)\n        \n        return self.final_tanh(out)\n\n# --- How to use it ---\n# Assuming stereo audio (2 channels)\nmodel = UNet(in_channels=2, out_channels=2)\n\n# Create a dummy input batch to test the model\n# (batch_size, num_channels, sequence_length)\ndummy_input = torch.randn(4, 2, 44100 * 2) # Batch of 4, 2-second stereo clips at 44.1kHz\n\noutput = model(dummy_input)\n\nprint(f\"Model created successfully!\")\nprint(f\"Input shape: {dummy_input.shape}\")\nprint(f\"Output shape: {output.shape}\")\n# Note: The output length might be slightly different due to convolutions.\n# You may need to pad the input or crop the output to ensure they match exactly.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T01:14:29.292239Z","iopub.execute_input":"2025-10-20T01:14:29.292597Z","iopub.status.idle":"2025-10-20T01:14:32.514715Z","shell.execute_reply.started":"2025-10-20T01:14:29.292565Z","shell.execute_reply":"2025-10-20T01:14:32.513820Z"}},"outputs":[{"name":"stdout","text":"Model created successfully!\nInput shape: torch.Size([4, 2, 88200])\nOutput shape: torch.Size([4, 2, 88200])\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import DataLoader\n\n# # --- 1. Setup and Hyperparameters ---\n\n# # Check for GPU availability and set the device\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {device}\")\n\n# # Hyperparameters\n# NUM_EPOCHS = 25\n# LEARNING_RATE = 1e-4 # A smaller learning rate is often good for U-Nets\n# BATCH_SIZE = 4       # Adjust based on your GPU memory\n# MANIFEST_FILE = \"/kaggle/working/dataset_manifest.csv\"\n# MODEL_SAVE_PATH = \"/kaggle/working/audio_upscaler_model.pth\"\n\n# # --- 2. Initialize Components ---\n\n# # Instantiate the Dataset and DataLoader\n# # (Assuming AudioUpscalingDataset and pad_collate_fn are defined in previous cells)\n# audio_dataset = AudioUpscalingDataset(manifest_path=MANIFEST_FILE, target_sample_rate=44100)\n# train_loader = DataLoader(\n#     audio_dataset, \n#     batch_size=BATCH_SIZE, \n#     shuffle=True,\n#     collate_fn=pad_collate_fn,\n#     num_workers=2 # Use multiple cores to load data faster\n# )\n\n# # Instantiate the Model and move it to the selected device\n# model = UNet(in_channels=2, out_channels=2).to(device)\n\n# # Instantiate the Loss Function and Optimizer\n# criterion = nn.L1Loss() # Mean Absolute Error is great for audio\n# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# print(\"Setup complete. Starting training...\")\n\n# # --- 3. The Training Loop ---\n\n# for epoch in range(NUM_EPOCHS):\n#     model.train()  # Set the model to training mode\n#     running_loss = 0.0\n    \n#     # Loop over the data loader\n#     for i, (inputs, targets) in enumerate(train_loader):\n#         # Move tensors to the configured device (GPU or CPU)\n#         inputs = inputs.to(device)\n#         targets = targets.to(device)\n        \n#         # --- Forward Pass ---\n#         # Get model outputs\n#         outputs = model(inputs)\n        \n#         # --- Crop Output ---\n#         # Ensure the output and target have the exact same length before calculating loss\n#         # This handles any minor size differences from the U-Net's convolutions.\n#         min_len = min(outputs.shape[2], targets.shape[2])\n#         outputs = outputs[:, :, :min_len]\n#         targets = targets[:, :, :min_len]\n        \n#         # --- Calculate Loss ---\n#         loss = criterion(outputs, targets)\n        \n#         # --- Backward Pass and Optimization ---\n#         # 1. Clear previous gradients\n#         optimizer.zero_grad()\n#         # 2. Calculate gradients\n#         loss.backward()\n#         # 3. Update model weights\n#         optimizer.step()\n        \n#         # --- Statistics ---\n#         running_loss += loss.item()\n        \n#         # Print progress for each batch\n#         if (i + 1) % len(train_loader) == 0: # Print at the end of each epoch\n#              print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], '\n#                    f'Batch [{i+1}/{len(train_loader)}], '\n#                    f'Loss: {loss.item():.4f}')\n\n#     # Print average loss for the epoch\n#     epoch_loss = running_loss / len(train_loader)\n#     print(f'--- End of Epoch [{epoch+1}/{NUM_EPOCHS}], Average Loss: {epoch_loss:.4f} ---')\n\n\n# print('Finished Training!')\n\n# # --- 4. Save the Trained Model ---\n# torch.save(model.state_dict(), MODEL_SAVE_PATH)\n# print(f\"Model saved to {MODEL_SAVE_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T18:11:17.215704Z","iopub.execute_input":"2025-10-18T18:11:17.216078Z","execution_failed":"2025-10-18T18:17:26.978Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Optimized training script (contains light U-net model using chunk of 2 sec audio)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchaudio\nimport pandas as pd\nimport numpy as np\n\n# --- (Optional but Recommended) Lighter U-Net Model ---\n# Using a model with fewer channels reduces memory usage significantly.\nclass LightUNet(nn.Module):\n    def __init__(self, in_channels=2, out_channels=2):\n        super().__init__()\n        # A simplified U-Net with fewer channels\n        self.enc1 = nn.Conv1d(in_channels, 16, kernel_size=15, padding=7)\n        self.pool1 = nn.MaxPool1d(2)\n        self.enc2 = nn.Conv1d(16, 32, kernel_size=15, padding=7)\n        self.pool2 = nn.MaxPool1d(2)\n        \n        self.bottleneck = nn.Conv1d(32, 64, kernel_size=15, padding=7)\n        \n        self.upconv2 = nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1)\n        self.dec2 = nn.Conv1d(64, 32, kernel_size=5, padding=2) # 32 + 32\n        self.upconv1 = nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1)\n        self.dec1 = nn.Conv1d(32, 16, kernel_size=5, padding=2) # 16 + 16\n        \n        self.final_conv = nn.Conv1d(16, out_channels, kernel_size=1)\n        self.final_tanh = nn.Tanh()\n\n    def forward(self, x):\n        e1 = self.enc1(x)\n        p1 = self.pool1(e1)\n        e2 = self.enc2(p1)\n        p2 = self.pool2(e2)\n        b = self.bottleneck(p2)\n        u2 = self.upconv2(b)\n        d2 = self.dec2(torch.cat([u2, e2], dim=1))\n        u1 = self.upconv1(d2)\n        d1 = self.dec1(torch.cat([u1, e1], dim=1))\n        return self.final_tanh(self.final_conv(d1))\n\n# --- NEW Dataset that uses fixed-size chunks ---\nclass AudioChunkDataset(Dataset):\n    def __init__(self, manifest_path, sample_rate=44100, chunk_duration_secs=2):\n        self.manifest = pd.read_csv(manifest_path)\n        self.sample_rate = sample_rate\n        self.chunk_size = sample_rate * chunk_duration_secs\n\n    def __len__(self):\n        return len(self.manifest)\n\n    def __getitem__(self, idx):\n        input_path = self.manifest.iloc[idx]['input_path']\n        target_path = self.manifest.iloc[idx]['target_path']\n        \n        input_waveform, _ = torchaudio.load(input_path)\n        target_waveform, _ = torchaudio.load(target_path)\n        \n        # Get a random chunk\n        # If the file is shorter than the chunk size, it will be padded later.\n        if input_waveform.shape[1] > self.chunk_size:\n            start = np.random.randint(0, input_waveform.shape[1] - self.chunk_size)\n            input_chunk = input_waveform[:, start:start + self.chunk_size]\n            target_chunk = target_waveform[:, start:start + self.chunk_size]\n        else:\n            input_chunk = input_waveform\n            target_chunk = target_waveform\n\n        # Pad if the chunk (or original file) is shorter than the desired chunk size\n        pad_len = self.chunk_size - input_chunk.shape[1]\n        if pad_len > 0:\n            input_chunk = torch.nn.functional.pad(input_chunk, (0, pad_len))\n            target_chunk = torch.nn.functional.pad(target_chunk, (0, pad_len))\n            \n        return input_chunk, target_chunk\n\n# --- 1. Setup and Hyperparameters ---\ndevice = torch.device(\"cpu\") # Forcing CPU\nprint(f\"Using device: {device}\")\n\n# Hyperparameters optimized for low memory\nNUM_EPOCHS = 25\nLEARNING_RATE = 1e-4\nBATCH_SIZE = 1 # Process one file at a time\nACCUMULATION_STEPS = 4 # Simulate a batch size of 1 * 4 = 4\nMANIFEST_FILE = \"/kaggle/working/dataset_manifest.csv\"\nMODEL_SAVE_PATH = \"/kaggle/working/audio_upscaler_cpu_model.pth\"\n\n# --- 2. Initialize Components ---\n# Use the new chunk-based dataset\naudio_dataset = AudioChunkDataset(manifest_path=MANIFEST_FILE)\n# No custom collate_fn needed! num_workers=0 is safer for low-memory.\ntrain_loader = DataLoader(audio_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n\n# Use the lighter model\nmodel = LightUNet(in_channels=2, out_channels=2).to(device)\ncriterion = nn.L1Loss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\nprint(\"Setup complete. Starting memory-efficient training...\")\n\n# --- 3. The UPDATED Training Loop with Gradient Accumulation ---\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    running_loss = 0.0\n    \n    # Reset gradients at the start of the epoch\n    optimizer.zero_grad()\n    \n    for i, (inputs, targets) in enumerate(train_loader):\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        \n        # Forward Pass\n        outputs = model(inputs)\n        \n        # Crop output to match target length precisely\n        min_len = min(outputs.shape[2], targets.shape[2])\n        outputs = outputs[:, :, :min_len]\n        targets = targets[:, :, :min_len]\n        \n        loss = criterion(outputs, targets)\n        \n        # Scale the loss for accumulation\n        loss = loss / ACCUMULATION_STEPS\n        \n        # Backward Pass\n        loss.backward()\n        \n        # --- Gradient Accumulation Step ---\n        # Update weights only every ACCUMULATION_STEPS\n        if (i + 1) % ACCUMULATION_STEPS == 0:\n            optimizer.step()  # Update weights\n            optimizer.zero_grad() # Reset gradients for the next accumulation cycle\n\n        running_loss += loss.item() * ACCUMULATION_STEPS # Un-scale for logging\n\n    # Print average loss for the epoch\n    epoch_loss = running_loss / len(train_loader)\n    print(f'--- End of Epoch [{epoch+1}/{NUM_EPOCHS}], Average Loss: {epoch_loss:.4f} ---')\n\nprint('Finished Training!')\n\n# --- 4. Save the Trained Model ---\ntorch.save(model.state_dict(), MODEL_SAVE_PATH)\nprint(f\"Model saved to {MODEL_SAVE_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T01:14:40.437647Z","iopub.execute_input":"2025-10-20T01:14:40.438440Z","iopub.status.idle":"2025-10-20T01:21:27.353282Z","shell.execute_reply.started":"2025-10-20T01:14:40.438411Z","shell.execute_reply":"2025-10-20T01:21:27.352326Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nSetup complete. Starting memory-efficient training...\n--- End of Epoch [1/25], Average Loss: 0.1361 ---\n--- End of Epoch [2/25], Average Loss: 0.1304 ---\n--- End of Epoch [3/25], Average Loss: 0.1299 ---\n--- End of Epoch [4/25], Average Loss: 0.1251 ---\n--- End of Epoch [5/25], Average Loss: 0.1186 ---\n--- End of Epoch [6/25], Average Loss: 0.1139 ---\n--- End of Epoch [7/25], Average Loss: 0.1111 ---\n--- End of Epoch [8/25], Average Loss: 0.1058 ---\n--- End of Epoch [9/25], Average Loss: 0.0983 ---\n--- End of Epoch [10/25], Average Loss: 0.0946 ---\n--- End of Epoch [11/25], Average Loss: 0.0885 ---\n--- End of Epoch [12/25], Average Loss: 0.0829 ---\n--- End of Epoch [13/25], Average Loss: 0.0781 ---\n--- End of Epoch [14/25], Average Loss: 0.0672 ---\n--- End of Epoch [15/25], Average Loss: 0.0550 ---\n--- End of Epoch [16/25], Average Loss: 0.0476 ---\n--- End of Epoch [17/25], Average Loss: 0.0426 ---\n--- End of Epoch [18/25], Average Loss: 0.0327 ---\n--- End of Epoch [19/25], Average Loss: 0.0311 ---\n--- End of Epoch [20/25], Average Loss: 0.0530 ---\n--- End of Epoch [21/25], Average Loss: 0.0389 ---\n--- End of Epoch [22/25], Average Loss: 0.0382 ---\n--- End of Epoch [23/25], Average Loss: 0.0250 ---\n--- End of Epoch [24/25], Average Loss: 0.0369 ---\n--- End of Epoch [25/25], Average Loss: 0.0324 ---\nFinished Training!\nModel saved to /kaggle/working/audio_upscaler_cpu_model.pth\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"Performing inference and then listening to the output","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchaudio\nimport numpy as np\n\n# --- 1. Setup ---\ndevice = torch.device(\"cpu\")\nMODEL_PATH = \"/kaggle/working/audio_upscaler_cpu_model.pth\"\n# Use one of your original MP3 files as input\nINPUT_AUDIO_PATH = \"/kaggle/working/mp3_converted/a1.mp3\" \nOUTPUT_AUDIO_PATH = \"/kaggle/working/upscaled_output.wav\"\nSAMPLE_RATE = 44100\nCHUNK_DURATION_SECS = 2 # Use the same chunk size as in training\nCHUNK_SIZE = SAMPLE_RATE * CHUNK_DURATION_SECS\n\n# --- 2. Load Model ---\n# Make sure the LightUNet class is defined in a previous cell\nmodel = LightUNet(in_channels=2, out_channels=2).to(device)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=device))\nmodel.eval() # Set the model to evaluation mode (very important!)\n\nprint(\"Model loaded. Starting inference...\")\n\n# --- 3. Load and Process Audio in Chunks ---\ninput_waveform, _ = torchaudio.load(INPUT_AUDIO_PATH)\ninput_waveform = input_waveform.to(device)\noutput_chunks = []\n\n# Process the audio chunk by chunk to avoid memory errors\nwith torch.no_grad(): # Disable gradient calculation for efficiency\n    for i in range(0, input_waveform.shape[1], CHUNK_SIZE):\n        chunk = input_waveform[:, i:i + CHUNK_SIZE]\n        \n        # Pad the last chunk if it's smaller than the required size\n        if chunk.shape[1] < CHUNK_SIZE:\n            pad_len = CHUNK_SIZE - chunk.shape[1]\n            chunk = torch.nn.functional.pad(chunk, (0, pad_len))\n\n        # Add a batch dimension and run through the model\n        chunk = chunk.unsqueeze(0) # Shape: [1, num_channels, chunk_size]\n        output_chunk = model(chunk)\n        output_chunks.append(output_chunk.squeeze(0)) # Remove batch dimension\n\n# --- 4. Stitch Chunks Together and Save ---\n# Concatenate all the processed chunks\noutput_waveform = torch.cat(output_chunks, dim=1)\n\n# Trim any excess padding from the end by matching the original input length\noutput_waveform = output_waveform[:, :input_waveform.shape[1]]\n\n# Save the final upscaled audio\ntorchaudio.save(OUTPUT_AUDIO_PATH, output_waveform.cpu(), SAMPLE_RATE)\n\nprint(f\"Inference complete! Upscaled audio saved to: {OUTPUT_AUDIO_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T01:44:59.967568Z","iopub.execute_input":"2025-10-20T01:44:59.967882Z","iopub.status.idle":"2025-10-20T01:45:14.055865Z","shell.execute_reply.started":"2025-10-20T01:44:59.967860Z","shell.execute_reply":"2025-10-20T01:45:14.054746Z"}},"outputs":[{"name":"stdout","text":"Model loaded. Starting inference...\nInference complete! Upscaled audio saved to: /kaggle/working/upscaled_output.wav\n","output_type":"stream"}],"execution_count":13}]}